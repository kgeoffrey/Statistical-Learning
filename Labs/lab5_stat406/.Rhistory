e.meanafter <- mean(exper[,2])
c.meanbefore <- mean(control[,1])
c.meanafter <- mean(control[,2])
e.dif <- exper[,1]-exper[,2]
e.meandif <- mean(e.dif)
e.meandif
rm(list=ls())
## Question 1
exper <- read.table("~/Dropbox/School/2018W1/STAT_404/webwork/webwork1_stat404/exper.txt", quote="\"", comment.char="")
control <- read.table("~/Dropbox/School/2018W1/STAT_404/webwork/webwork1_stat404/control.txt", quote="\"", comment.char="")
n.e <- dim(exper)[1]
n.c <- dim(control)[1]
a <- dim(exper)[2]
N <- a*(n.e+n.c)
e.before <- exper[,1]
e.after <- exper[,2]
c.before <- control[,1]
c.after <- control[,2]
e.meanbefore <- mean(exper[,1])
e.meanafter <- mean(exper[,2])
c.meanbefore <- mean(control[,1])
c.meanafter <- mean(control[,2])
e.dif <- exper[,1]-exper[,2]
e.meandif <- mean(e.dif)
e.meandif
e.difsd <- sd(e.dif)
e.difSE <- e.difsd/sqrt(n)
e.difSE
e.difSE <- e.difsd/sqrt(n.e)
e.difSE
e.residbefore <- e.before-e.meanbefore
c.residbefore <- c.before-c.meanbefore
c.difsd <- sd(c.dif)
s2 <- ((sum(e.residbefore))^2 + (sum(c.residbefore))^2)/(N-a)
e.meanbefore-c.meanbefore
s2.before <- ((sum(e.residbefore))^2 + (sum(c.residbefore))^2)/(N-a)
ec.meanbefore <- e.meanbefore-c.meanbefore
ec.SEbefore <- sqrt(s2.before)*sqrt(1/n.e+1/n.c)
ec.SEbefore
df2 <- N-a
df2
c.dif <- control[,1]-control[,2]
c.meandif <- mean(c.dif)
c.meandif
# Part 1c
teststat.1c <- e.meandif - c.meandif
# Part 1c
e.residdif <- e.dif - e.meandif
c.residdif <- c.dif - c.meandif
# Part 1c
e.residdif <- e.dif - e.meandif
c.residdif <- c.dif - c.meandif
teststat.1c <- e.meandif - c.meandif
s2.ecdif <- ((sum(e.residdif))^2+(sum(c.residdif))^2)/(N-a)
teststat.1c
ec.SEdif <- sqrt(s2.ecdif)*sqrt(1/n.e+1/n.c)
ec.SEdif <- sqrt(s2.ecdif)*sqrt(1/n.e+1/n.c)
ec.SEdif
e.dif <- e.meanbefore - e.meanafter
e.meandif <- mean(e.dif)
e.meandif
e.meanafter-e.meanbefore
e.dif <- e.before - e.after
e.meandif <- mean(e.dif)
e.meandif
t.test(e.before, e.after, paired=TRUE)
t.test(e.after, e.before, paired=TRUE)
summary(t.test(e.after, e.before, paired=TRUE))
t.test(e.after, e.before, paired=TRUE)
e.dif <- e.after - e.before
e.difsd <- sd(e.dif)
e.difSE <- e.difsd/sqrt(n.e)
e.difSE
# 1b)
t.test(e.before, c.before, var.equal=TRUE)
# 1b)
test1b <- t.test(e.before, c.before, var.equal=TRUE)
meandif1b <- abs(23.30833-20.74545)
meandif1b
se1b <- (9.682032-meandif1b)/0.4624
se1b
e.dif <- e.after - e.before
c.dif <- c.after - c.before
e.meanbefore <- mean(exper[,1])
e.meanafter <- mean(exper[,2])
c.meanbefore <- mean(control[,1])
c.meanafter <- mean(control[,2])
e.dif <- e.after - e.before
e.meandif <- mean(e.dif)
e.meandif
c.dif <- c.after - c.before
c.meandif <- mean(c.dif)
c.meandif
e.difsd <- sd(e.dif)
c.difsd <- sd(c.dif)
# 1c)
test1c <- t.test(c.dif, e.dif, alternative="two.sided", var.equal = FALSE)
test1c
meandif1c <- abs(-0.2454545 - -3.0833333)
meandif1c
se1c <- (4.7266604-meandif1c)/3.2625
se1c
# 1b)
test1b <- t.test(e.before, c.before, var.equal=TRUE)
meandif1b
test1b
qt(c(.025, .975), df=21)
se1b <- (9.682032-meandif1b)/2.079614
se1b
dt(2.160369)
pt(2.160369)
pt(2.160369, 12)
pt(2.160369, 21)
qt(c(.025, .975), df=12)
qt(c(.025, .975), df=12.38)
test1c
se1c <- (4.7266604-meandif1c)/2.160369
se1c
train <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/training.txt", quote="\"", comment.char="")
test <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/testing.txt", quote="\"", comment.char="")
rm(list=ls())
train <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/training.txt", quote="\"", comment.char="")
test <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/testing.txt", quote="\"", comment.char="")
p <- ncol(train)
ncol(test)
View(train)
View(train)
train <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/training.txt", quote="\"", comment.char="", header = TRUE)
test <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/testing.txt", quote="\"", comment.char="", header = TRUE)
null <- lm(MORT~1, data=train)
full <- lm(MORT~., data=train)
cor(train)
cor(train)
cor(train)
cor(train)
cor(train)
cor(train)
cor(train)
reduced <- lm(MORT~POOR+HC+NOX+HOUS+NONW, data=train)
k <- 5
n <- nrow(train)+nrow(test)
pr.full <- pr.reduced <- rep(0, n)
pr.full <- predict(fulltrain, newdata=test)
pr.full <- pr.reduced <- rep(0, n)
n <- nrow(train)+nrow(test)
k <- 5
for (i in 1:k) {
fulltrain <- lm(MORT~., data=train)
reducedtrain <- lm(MORT~POOR+HC+NOX+HOUS+NONW, data=train)
pr.full <- predict(fulltrain, newdata=test)
pr.reduced <- predict(partialtrain, newdata=test)
}
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
mspe_full
mspe_reduced <- mspe_full <- mean((train$MORT-pr.reduced)^2, (test$MORT-pr.reduced)^2)
mspe_full
train <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/training.txt", quote="\"", comment.char="", header = TRUE)
test <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/testing.txt", quote="\"", comment.char="", header = TRUE)
# p <- ncol(train)
# nulltrain <- lm(MORT~1, data=train)
fulltrain <- lm(MORT~., data=train)
reducedtrain <- lm(MORT~POOR+HC+NOX+HOUS+NONW, data=train)
pr.full <- predict(fulltrain, newdata=test)
pr.reduced <- predict(reducedtrain,)
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
mspe_full
mspe_reduced <- mspe_full <- mean((train$MORT-pr.reduced)^2, (test$MORT-pr.reduced)^2)
mspe_full
train <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/training.txt", quote="\"", comment.char="", header = TRUE)
test <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/testing.txt", quote="\"", comment.char="", header = TRUE)
# p <- ncol(train)
# nulltrain <- lm(MORT~1, data=train)
fulltrain <- lm(MORT~., data=train)
reducedtrain <- lm(MORT~POOR+HC+NOX+HOUS+NONW, data=train)
pr.full <- predict(fulltrain, newdata=test)
pr.reduced <- predict(reducedtrain,)
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
data <- merge(train, test)
data <- merge(train, test, by.x)
data <- merge(train, test, by.x=TRUE)
data <- merge(train, test, by.y=TRUE)
data <- merge(train, test, by.y="all")
data <- merge(train, test, by.x="all")
# p <- ncol(train)
# nulltrain <- lm(MORT~1, data=train)
fulltrain <- lm(MORT~., data=train)
reducedtrain <- lm(MORT~POOR+HC+NOX+HOUS+NONW, data=train)
pr.full <- predict(fulltrain, newdata=test)
pr.reduced <- predict(reducedtrain,)
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
mspe_full <- mean((train$MORT-pr.full)^2)
pr.reduced <- predict(reducedtrain, newdata=test)
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
mspe_full
mspe_reduced <- mspe_full <- mean((train$MORT-pr.reduced)^2, (test$MORT-pr.reduced)^2)
mspe_full
mspe_reduced
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
mspe_full
mspe_reduced <- mean((train$MORT-pr.reduced)^2, (test$MORT-pr.reduced)^2)
mspe_reduced
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
mspe_full
mspe_reduced <- mean((train$MORT-pr.reduced)^2, (test$MORT-pr.reduced)^2)
mspe_reduced
rm(list=ls())
train <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/training.txt", quote="\"", comment.char="", header = TRUE)
test <- read.table("~/Dropbox/School/2018W1/STAT_406/Code/quiz1/testing.txt", quote="\"", comment.char="", header = TRUE)
# p <- ncol(train)
# nulltrain <- lm(MORT~1, data=train)
fulltrain <- lm(MORT~., data=train)
reducedtrain <- lm(MORT~POOR+HC+NOX+HOUS+NONW, data=train)
pr.full <- predict(fulltrain, newdata=test)
pr.reduced <- predict(reducedtrain, newdata=test)
mspe_full <- mean((train$MORT-pr.full)^2, (test$MORT-pr.full)^2)
mspe_full
mspe_reduced <- mean((train$MORT-pr.reduced)^2, (test$MORT-pr.reduced)^2)
mspe_full <- mean((test$MORT - pr.full)^2)
mspe_full
mspe_reduced <- mean((test$MORT - pr.reduced)^2)
mspe_reduced
cor(train)
cor(train)[16]
cor(train)[,16]
t(cor(train)[,16])
sort(cor(train)[,16])
reduced2 <- lm(MORT~NONW+PREC+SO+POPN+DENS, data=train)
pr.reduced2 <- predict(reduced2, newdata=test)
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+PREC+SO+POPN, data=train)
pr.full <- predict(fulltrain, newdata=test)
pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
mspe_full <- mean((test$MORT - pr.full)^2)
mspe_full
mspe_reduced <- mean((test$MORT - pr.reduced)^2)
mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+PREC+SO+POPN, data=train)
# pr.full <- predict(fulltrain, newdata=test)
# pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
# mspe_full <- mean((test$MORT - pr.full)^2)
# mspe_full
# mspe_reduced <- mean((test$MORT - pr.reduced)^2)
# mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+PREC+SO, data=train)
# pr.full <- predict(fulltrain, newdata=test)
# pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
# mspe_full <- mean((test$MORT - pr.full)^2)
# mspe_full
# mspe_reduced <- mean((test$MORT - pr.reduced)^2)
# mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+PREC, data=train)
# pr.full <- predict(fulltrain, newdata=test)
# pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
# mspe_full <- mean((test$MORT - pr.full)^2)
# mspe_full
# mspe_reduced <- mean((test$MORT - pr.reduced)^2)
# mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+SO, data=train)
# pr.full <- predict(fulltrain, newdata=test)
# pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
# mspe_full <- mean((test$MORT - pr.full)^2)
# mspe_full
# mspe_reduced <- mean((test$MORT - pr.reduced)^2)
# mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+PREC+POPN, data=train)
# pr.full <- predict(fulltrain, newdata=test)
# pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
# mspe_full <- mean((test$MORT - pr.full)^2)
# mspe_full
# mspe_reduced <- mean((test$MORT - pr.reduced)^2)
# mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+PREC+DENS, data=train)
# pr.full <- predict(fulltrain, newdata=test)
# pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
# mspe_full <- mean((test$MORT - pr.full)^2)
# mspe_full
# mspe_reduced <- mean((test$MORT - pr.reduced)^2)
# mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
reduced2 <- lm(MORT~NONW+PREC+SO, data=train)
# pr.full <- predict(fulltrain, newdata=test)
# pr.reduced <- predict(reducedtrain, newdata=test)
pr.reduced2 <- predict(reduced2, newdata=test)
# mspe_full <- mean((test$MORT - pr.full)^2)
# mspe_full
# mspe_reduced <- mean((test$MORT - pr.reduced)^2)
# mspe_reduced
mspe_reduced2 <- mean((test$MORT - pr.reduced2)^2)
mspe_reduced2
mspe_full
mspe_reduced
install.packages("agricolae")
library(agricolae)
###Here you should see around 5\% of replications have a F-statistic larger than
###the 95th percentile of the corresponding F distribution.
###Try more simulation cases with different inputs to understand
# the variability as sample sizes increase etc.
rm(list=ls())
y1 <- runif(n = 20, min = 1, max = 50)
y2 <- runif(n = 20, min = 1, max = 50)
y3 <- runif(n = 20, min = 1, max = 50)
Y <- matrix(c(y1, y2, y3), ncol = 3, nrow = 20)
View(Y)
a <- 3
b <- 20
ybar_iplus <- c(mean(y1), mean(y2), mean(y3))
ybar_plusj <- rep(0, 20)
for (i in 1:20) {
ybar_plusj[i] <- mean(y1[i], y2[i], y3[i])
}
a*b
ybar_plusplus <- (1/a*b)*sum(c(y1, y2, y3))
ybar_plusplus <- (1/(a*b))*sum(c(y1, y2, y3))
sum(y1-ybar_iplus[1])
sum(y2-ybar_iplus[2])
sum(y3-ybar_iplus[3])
sum(sum(ybar_iplus[1]-ybar_plusplus), sum(ybar_iplus[2]-ybar_plusplus), sum(ybar_iplus[3]-ybar_plusplus))
# a (FALSE)
summ <- 0
for (i in 1:a) {
for (j in 1:b) {
summ <- summ + (ybar_iplus[i] - ybar_plusplus)*(Y[j, i] - ybar_iplus[i])
}
}
# b (FALSE)
summ <- 0
for (i in 1:a) {
for (j in 1:b) {
summ <- summ + (ybar_plusj[j] - ybar_plusplus)*(Y[j, i] - ybar_plusj[j])
}
}
View(Y)
y1 <- runif(n = b, min = 1, max = 30)
y2 <- runif(n = b, min = 1, max = 30)
y3 <- runif(n = b, min = 1, max = 30)
Y <- matrix(c(y1, y2, y3), ncol = a, nrow = b)
ybar_iplus <- c(mean(y1), mean(y2), mean(y3))
ybar_plusj <- rep(0, b)
for (i in 1:b) {
ybar_plusj[i] <- mean(y1[i], y2[i], y3[i])
}
ybar_plusplus <- (1/(a*b))*sum(c(y1, y2, y3))
sum(y1-ybar_iplus[1])
sum(y2-ybar_iplus[2])
sum(y3-ybar_iplus[3])
sum(sum(ybar_iplus[1]-ybar_plusplus), sum(ybar_iplus[2]-ybar_plusplus), sum(ybar_iplus[3]-ybar_plusplus))
# a (FALSE)
summ <- 0
for (i in 1:a) {
for (j in 1:b) {
summ <- summ + (ybar_iplus[i] - ybar_plusplus)*(Y[j, i] - ybar_iplus[i])
}
}
# b (FALSE)
summ <- 0
for (i in 1:a) {
for (j in 1:b) {
summ <- summ + (ybar_plusj[j] - ybar_plusplus)*(Y[j, i] - ybar_plusj[j])
}
}
# c (TRUE)
summ1 <- 0
summ2 <- 0
for (i in 1:a) {
summ1 <- summ1 + sum(ybar_iplus[i]-ybar_plusplus)
for (j in 1:b) {
summ2 <- summ2 + sum(Y[j, i] - ybar_iplus[i])
}
}
summ1*summ2
setwd("~/Dropbox/School/2018W1/STAT_406/Labs/lab5_stat406/")
library(MASS)
read.table("CarData", header = TRUE)
data <- read.table("CarData", header = TRUE)
View(data)
View(data)
data <- read("CarData", header = TRUE)
data <- load("CarData", header = TRUE)
data <- load("CarData")
data
car.data
load("CarData")
rm(data)
View(car.data)
dat <- load("CarData")
library(rpart)
?rpart
tree1 <- rpart(Mileage~., data = dat, control = rpart.control(minsplit=5, cp=01))
tree1 <- rpart(Mileage~., data = car.data, control = rpart.control(minsplit=5, cp=01))
plot(tree1)
tree1 <- rpart(Mileage~., data = car.data, method = "anova", control = rpart.control(minsplit=5, cp=01))
plot(tree1)
plot(tree1, uniform = FALSE, margin = 0.05)
tree1 <- rpart(Mileage~., data = car.data, control = rpart.control(minsplit=5, cp=01))
plot(tree1, uniform = FALSE, margin = 0.05)
tree1 <- rpart(Mileage~., data = car.data, method = "anova", control = rpart.control(minsplit=5, cp=01))
plot(tree1, uniform = FALSE, margin = 0.05)
car.data
tree1 <- rpart(Mileage~., data = car.data, method = "anova", control = rpart.control(minsplit=5, cp=0.01))
plot(tree1, uniform = FALSE, margin = 0.05)
tree1
tree1 <- rpart(Mileage~., data = car.data, method = "anova", control = rpart.control(minsplit=5, cp=0.01))
plot(tree1, uniform = FALSE, margin = 0.05)
tree1 <- rpart(Mileage~., data = car.data, control = rpart.control(minsplit=5, cp=0.01))
plot(tree1, uniform = FALSE, margin = 0.05)
mse <- mean((car.data$Mileage-predict(tree1))^2)
mse
tree1 <- rpart(Mileage~., data = car.data, method = "anova", control = rpart.control(minsplit=5, cp=0.01))
mse <- mean((car.data$Mileage-predict(tree1))^2)
mse
tree1 <- rpart(Mileage~., data = car.data, control = rpart.control(minsplit=5, cp=0.01))
mse <- mean((car.data$Mileage-predict(tree1))^2)
mse
tree2 <- rpart(Mileage~., data = car.data, method = "anova", control = rpart.control(minsplit=20, cp=0.01))
plot(tree2)
mse2 <- mean((car.data$Mileage-predict(tree2))^2)
mse2
## Question 5
set.seed(400)
gpind <- sample(rep(1:7, each=7))
k <- 7
N <- nrow(car.data)
ii <- (1:N) %% k + 1
ii
sample(ii)
?sample
mspe.n <- mspe.st <- rep(0, N)
for (i in 1:N) {
ii <- sample(ii)
pr.n <- pr.st <- rep(0, N)
for (j in 1:k) {
tmp.st <- update(tree2, data=car.data[ii != j, ])
pr.st[ii == j] <- predict(tmp.st, newdata=car.data[ii == j, ])
pr.n[ii == j] <- with(car.data[ii != j, ], mean(car.data$Mileage))
}
mspe.st[i] <- with(car.data, mean((car.data$Mileage - pr.st)^2))
mspe.n[i] <- with(car.data, mean((car.data$Mileage - pr.n)^2))
}
summary(mspe.st)
summary(mspe.n)
for (i in 1:N) {
ii <- sample(ii)
pr.n <- pr.st <- rep(0, N)
for (j in 1:k) {
tmp.st <- update(tree1, data=car.data[ii != j, ])
pr.st[ii == j] <- predict(tmp.st, newdata=car.data[ii == j, ])
pr.n[ii == j] <- with(car.data[ii != j, ], mean(car.data$Mileage))
}
mspe.st[i] <- with(car.data, mean((car.data$Mileage - pr.st)^2))
mspe.n[i] <- with(car.data, mean((car.data$Mileage - pr.n)^2))
}
summary(mspe.st)
for (i in 1:N) {
ii <- sample(ii)
pr.n <- pr.st <- rep(0, N)
for (j in 1:k) {
# tmp.st <- update(tree1, data=car.data[ii != j, ])
tmp.st <- rpart(Mileage~., data = car.data[ii != j, ], method = "anova", control = rpart.control(minsplit=20, cp=0.01))
pr.st[ii == j] <- predict(tmp.st, newdata=car.data[ii == j, ])
pr.n[ii == j] <- with(car.data[ii != j, ], mean(car.data$Mileage))
}
mspe.st[i] <- with(car.data, mean((car.data$Mileage - pr.st)^2))
mspe.n[i] <- with(car.data, mean((car.data$Mileage - pr.n)^2))
}
summary(mspe.st)
for (i in 1:N) {
ii <- sample(ii)
pr.n <- pr.st <- rep(0, N)
for (j in 1:k) {
# tmp.st <- update(tree1, data=car.data[ii != j, ])
tmp.st <- rpart(Mileage~., data = car.data[ii != j, ], method = "anova", control = rpart.control(minsplit=5, cp=0.01))
pr.st[ii == j] <- predict(tmp.st, newdata=car.data[ii == j, ])
pr.n[ii == j] <- with(car.data[ii != j, ], mean(car.data$Mileage))
}
mspe.st[i] <- with(car.data, mean((car.data$Mileage - pr.st)^2))
mspe.n[i] <- with(car.data, mean((car.data$Mileage - pr.n)^2))
}
mspe.n <- mspe.st <- rep(0, N)
for (i in 1:N) {
ii <- sample(ii)
pr.n <- pr.st <- rep(0, N)
for (j in 1:k) {
# tmp.st <- update(tree1, data=car.data[ii != j, ])
tmp.st <- rpart(Mileage~., data = car.data[ii != j, ], method = "anova", control = rpart.control(minsplit=5, cp=0.01))
pr.st[ii == j] <- predict(tmp.st, newdata=car.data[ii == j, ])
pr.n[ii == j] <- with(car.data[ii != j, ], mean(car.data$Mileage))
}
mspe.st[i] <- with(car.data, mean((car.data$Mileage - pr.st)^2))
mspe.n[i] <- with(car.data, mean((car.data$Mileage - pr.n)^2))
}
summary(mspe.st)
